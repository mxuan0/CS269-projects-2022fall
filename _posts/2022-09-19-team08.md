---
layout: post
comments: true
title: The Emergence of Roles in Multi-agent Traffic Learning
author: Minglu Zhao, Jingdong Gao (Team 08)
date: 2022-09-19
---


> We are interested in applying role-based multi-agent reinforcement learning (MARL) to self-driven particle systems. Specifically, we are inspired by role-based hierarchical MARL frameworks (Wang et al., 2020a; Wang et al., 2020b), which explicitly models agents as having different roles, with each role having its own policy. In this way, the search space for the policy is greatly reduced as the joint action space for roles is much smaller than the joint action space for all agents. We hope to further explore this idea in the MetaDrive environment to study the emergence of roles in the traffic setting. In conclusion, our group hopes to apply various hierarchy-based MARL algorithms to autonomous driving scenarios, and compare our results with existing methods such as (Peng et al., 2021). 
<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## What is role-based MARL

### The MARL Background 
Multi-agent collaboration scenarios are commonly observed in real-world applications, such as  autonomous vehicle teams (Cao et al., 2012), intelligent warehouse systems (Nowe et al., 2012), and sensor networks (Zhang & Lesser, 2011). In recent years, multi-agent reinforcement learning (MARL), combined with deep-learning-based methods, has achieved prominent progress in simulating efficient collaborative behavior. However, one critical issue often encountered in multi-agent scenarios is the dimensionality issue -- the observation space and action space increase exponentially with increased number of agents, putting great burden on the value and policy network training and thus negatively affect the efficiency. To deal with such issues, many recent methods apply a centralized training with decentralized execution (CTDE) framework. With CTDE, joint information regarding all agents is fed into the joint (centralized) evaluation network to ease training, while the evaluation is then decomposed into individual (decentralized) modes following which agents aim to improve their policies. 

### The Motivation for Roles
On the other hand, the majority of CTDE-based algorithms rely on deep neural networks to decompose the central evaluation with no specific guidance on how to decompose the team value meaningfully. The decomposition process remains a black box, which makes it hard to ensure these algorithms can achieve efficient cooperative behavior as humans do. With this mindset, our group explored a paper regarding role-based MARL: to learn roles for the task to guide agents’ behavior [5]. Under the framework of RODE, agents are explicitly modeled as having different roles, with each role corresponding to one specific policy. In this way, the search space for the policy is greatly reduced as the joint action space for roles is much smaller than the joint action space for all agents. At the same time, explicitly modeling roles can generate more interpretable policies, which resonates with real-life human cooperation. We then adapt and apply the framework to multi-agent autonomous driving scenarios, aiming to generate effective cooperation under various difficult traffic conditions. We further apply state-of-the-art MARL algorithms [4][7] as baselines to experiment on the effectiveness of role-based representations in traffic scenarios. 


## Our Approach
action based roles

more specific details about RODE, action encoder, role selector, role policy 

![RODE]({{ '/assets/images/team08/rode-arch.png' | relative_url }})
{: style="width: 900px; max-width: 100%;"}
*Fig 1. RODE framework. (a) The forward model for learning action representations. (b) Role
selector architecture. (c) Role action spaces and role policy structure.* [5].

collaborative in our setting


## Experiments

To study the emergence of roles in traffic learning settings, we apply the RODE framework to MetaDrive environments. Currently, we test our implementation on two existing MARL environments in MetaDrive, Roundabout and Bottleneck. We study the cooperative setting via summing individual agent rewards as the global environment reward and set the environment termination status once a single vehicle terminates. Since RODE assumes discrete agent action spaces, we discretize agents' action spaces and set both steering and throttle dimensions to 5, which lead to a similar number of actions for each agent to RODE's testing environment. During training, we set the maximum length of each episode to 200, and the training duration of the action encoder to 50000 environment steps.

In the current stage, we have not successfully made the training converge, while we have observed patterns from past training processes that may help address the issue. The prediction loss for rewards stops decreasing before the training for the action encoder completes and the prediction loss for observations is still decreasing when action encoder training stops. One potential solution is to increase the training length and capacity of the action encoder. Another main observation is that the training loss for the role and action selectors seem to increase after a short period of decreasing. While constrained with computation resources, we need to run with more environment steps to confirm this issue. Currently we are running 250000 steps while the original RODE trains 2 million steps in the StarCraft II environment. One reason for this might be that the role of an agent changes faster in the traffic setting so we need to reselect a role after each environment step, whereas our current training process changes an agent’s role every 5 steps. 

In general, our next steps have to overcome the challenge of finding more computation resources, for either training with more steps, using larger networks, tuning hyper-parameters, or changing the roles more frequently. After successfully running RODE to convergence on Metadrive, we plan to compare the results to a discrete version of COPO to make the comparison more consistent, and study the resulting behaviors in the traffic system between two frameworks.

## Relevant papers:
[1] Cao, Y., Yu, W., Ren, W., and Chen, G. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1):427–438, 2012.

[2] Zhang, C. and Lesser, V. Coordinated multi-agent reinforcement learning in networked distributed pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.

[3] Nowe, A., Vrancx, P., and De Hauwere, Y.-M. Game theory and multi-agent reinforcement learning. In ReinforcementLearning, pp. 441–470. Springer, 2012.

[4] Learning to Simulate Self-driven Particles System with Coordinated Policy Optimization
https://arxiv.org/pdf/2110.13827.pdf

[5] Rode: Learning roles to decompose multi-agent tasks
https://arxiv.org/pdf/2010.01523.pdf

[6] Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement learning with emergent roles. In Proceedings of the 37th International Conference on Machine Learning, 2020c.

[7] Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and Whiteson, S. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4295–4304, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018.
